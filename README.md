# KI-gest√ºtzte Vertragsanalyse

Ein fortschrittliches System zur automatisierten Analyse von Vertr√§gen mit KI und juristischen Wissensbasen.

## Projekt√ºbersicht

Dieses System erm√∂glicht eine detaillierte Analyse von Vertr√§gen durch:

1. **Inkrementelle Strukturierung**: Umwandlung unstrukturierter Vertragstexte in strukturierte Markdown-Elemente.
2. **Intelligentes Chunking**: Gruppierung der Vertragselemente in logische Analyse-Einheiten.
3. **Kontextualisierte Klauselanalyse**: Bewertung der Klauseln nach einem Ampelsystem (Rot/Gelb/Gr√ºn) mit juristischer Wissensbasis.
4. **Alternative Formulierungen**: Automatische Generierung von besseren Formulierungen f√ºr problematische Klauseln.
5. **KI-gest√ºtzte Optimierung**: M√∂glichkeit zur weiteren Verbesserung von Vertragsklauseln.

## Technische Architektur

Das System basiert auf folgenden Komponenten:

- **Frontend**: Next.js mit React und Tailwind CSS f√ºr die Benutzeroberfl√§che
- **Backend**: API-Routen mit Next.js
- **Wissensdatenbank**: Vektordatenbank mit Convex f√ºr semantische Suche
- **KI-Integration**: OpenAI (GPT-4 Turbo, Embeddings API)

## Vertragsanalyse-Prozess

1. **Strukturierung**: Ein Vertrag wird durch KI in strukturierte Elemente umgewandelt.
2. **Analyse**: Jede Vertragsklausel wird:
   - Vektorisiert und semantisch in der Wissensdatenbank gesucht
   - Auf Basis relevanter juristischer Regeln und Gesetze analysiert
   - Nach einem Ampelsystem kategorisiert:
     - üî¥ **Rot**: Kritische Klausel, ablehnen oder grundlegend √ºberarbeiten
     - üü° **Gelb**: Verhandelbare Klausel, Anpassungen erforderlich
     - üü¢ **Gr√ºn**: Akzeptable Klausel
3. **Alternativvorschl√§ge**: F√ºr kritische oder verhandelbare Klauseln werden bessere Formulierungen vorgeschlagen.
4. **Optimierung**: Der Benutzer kann eigene Formulierungen eingeben und mit KI optimieren lassen.

## Wissensbasis

Die Wissensbasis besteht aus:

- Juristischen Analysen des deutschen und √∂sterreichischen Vertragsrechts
- Regeln f√ºr die Bewertung von Klauseln, kategorisiert nach Schweregrad
- Gerichtsentscheidungen und Expertenmeinungen

## Installation

```bash
# Repository klonen
git clone https://github.com/username/vertragsanalyse.git
cd vertragsanalyse

# Abh√§ngigkeiten installieren
npm install

# Umgebungsvariablen einrichten
cp .env.example .env.local
# Dann .env.local bearbeiten und API-Schl√ºssel eintragen

# Entwicklungsserver starten
npm run dev
```

## Umgebungsvariablen

Folgende Umgebungsvariablen werden ben√∂tigt:

```
GOOGLE_API_KEY=your-google-api-key
OPENAI_API_KEY=your-openai-key
CONVEX_DEPLOYMENT=your-convex-deployment-url
```

Die `GOOGLE_API_KEY` wird f√ºr die Gemini-Embeddings verwendet, w√§hrend die `OPENAI_API_KEY` weiterhin in den Frontend-API-Routen f√ºr die Vertragsstrukturierung und -analyse genutzt wird.

## Aktuelle KI-Modell-Konfiguration und Prototyping-Hinweise (Stand: Entwicklung Stufe 0)

Dieser Abschnitt dokumentiert die aktuelle Konfiguration der KI-Modelle und die Strategien zur Handhabung von API-Limits w√§hrend der Prototyping-Phase. Ziel ist es, sp√§ter auf robustere L√∂sungen (z.B. Google Cloud Vertex AI) umzusteigen.

### 1. Embedding-Modell (f√ºr Wissensdatenbank & Klausel-Vektorisierung)

- **Modell:** Google Gemini API `models/text-embedding-004`
- **API-Schl√ºssel:** `GOOGLE_API_KEY` (aus `.env` / Convex Umgebungsvariablen)
- **Begr√ºndung:** Nutzung der kostenlosen Stufe f√ºr initiales Prototyping und Bef√ºllung der Wissensdatenbank.
- **Bekannte Limits (Hard):**
    - Eingabe-Token-Limit pro Text: 2.048 Tokens.
    - Ausgabe-Dimension: 768.
    - API-Ratenbegrenzungen (RPM/RPD): Die genauen Limits der kostenlosen Stufe sind nicht explizit dokumentiert und m√ºssen im Auge behalten werden.
- **Implementierte Gegenma√ünahmen / Strategien:**
    - **Batching & Delay:** Die Convex Action `getEmbeddings` in `convex/knowledgeBase.ts` teilt Anfragen in Sub-Batches auf (`MAX_TEXTS_PER_EMBEDDING_BATCH`, Standard: 100 Texte) und f√ºgt eine konfigurierbare Verz√∂gerung (`DELAY_BETWEEN_EMBEDDING_BATCHES_MS`, Standard: 1000ms) zwischen den Batches ein, um Ratenlimits nicht zu √ºberschreiten.
    - **Token-Warnung:** Eine grobe Sch√§tzung der Zeichenl√§nge (`MAX_CHARS_PER_TEXT_CHUNK`) warnt in der Konsole, wenn einzelne Texte das 2048-Token-Limit √ºberschreiten k√∂nnten. *Hinweis: Die Texte sollten idealerweise vorab passend aufbereitet/gek√ºrzt werden.*
    - **Retry-Mechanismus:** API-Aufrufe nutzen die Hilfsfunktion `fetchWithRetry` (siehe unten).
- **Zuk√ºnftige Umstellung:** F√ºr den Produktiveinsatz und zur Aufhebung der Limitierungen ist eine Migration zu Google Cloud Vertex AI Embeddings geplant.

### 2. Textgenerierungs- / Analyse-Modelle (f√ºr Strukturierung, Klauselanalyse, etc.)

- **Geplantes Modell (ab Stufe 1 des Gesamtplans):** Google Gemini API `gemini-2.0-flash-001` (oder Nachfolger)
- **API-Schl√ºssel:** `GOOGLE_API_KEY` (aus `.env` / Convex Umgebungsvariablen)
- **Begr√ºndung:** Gutes Verh√§ltnis von Kosten, Geschwindigkeit und Leistung f√ºr komplexe Textaufgaben.
- **Bekannte Limits (Hard):**
    - Anfragen pro Minute (RPM): 15
    - Tokens pro Minute (TPM): 1.000.000
    - Anfragen pro Tag (RPD): 1.500
    - Eingabe-Token-Limit: 1.048.576
    - Ausgabe-Token-Limit: 8.192
- **Implementierte/Geplante Gegenma√ünahmen / Strategien:**
    - **Retry-Mechanismus:** Alle API-Aufrufe an dieses Modell sollen die Hilfsfunktion `fetchWithRetry` (siehe unten) verwenden.
    - **Prompt Engineering:** Sorgf√§ltige Gestaltung der System-Prompts, um die KI anzuweisen, innerhalb der Ausgabe-Token-Limits zu bleiben und pr√§zise Antworten zu generieren.
    - **Eingabe-Management:** Die Logik, die Kontext und Anfragen f√ºr die KI zusammenstellt, muss das hohe Eingabe-Token-Limit im Auge behalten, aber auch auf Effizienz achten.
    - **Monitoring:** Die API-Nutzung sollte √ºber das Google Cloud Dashboard √ºberwacht werden, um die Einhaltung der RPD/TPM-Limits sicherzustellen.
    - **"Weiche Grenzen":** Als Designprinzip sollte versucht werden, nicht st√§ndig an den harten Limits zu operieren, sondern einen Puffer einzuplanen. Dies wird prim√§r durch das Anwendungsdesign und die Frequenz der Funktionsaufrufe gesteuert.
- **Zuk√ºnftige Umstellung:** F√ºr h√∂here Skalierbarkeit und ggf. Finetuning-M√∂glichkeiten ist eine Migration zu Google Cloud Vertex AI f√ºr diese Modelle ebenfalls eine Option.

### 3. Hilfsfunktion: `fetchWithRetry`

- **Standort:** `convex/utils/llmUtils.ts`
- **Zweck:** Stellt eine robuste Methode f√ºr API-Aufrufe bereit, die automatisch Wiederholungsversuche bei bestimmten transienten Fehlern durchf√ºhrt.
- **Funktionsweise:**
    - F√ºhrt einen `fetch`-Aufruf aus.
    - Bei HTTP-Statuscodes wie `429` (Too Many Requests), `500`, `502`, `503`, `504` oder bei Netzwerkfehlern wird ein Wiederholungsversuch gestartet.
    - Nutzt exponentiellen Backoff zwischen den Versuchen (beginnend mit `INITIAL_BACKOFF_MS`, maximal `MAX_BACKOFF_MS`).
    - F√ºgt einen "Jitter" (zuf√§llige Zeitabweichung) hinzu, um gleichzeitige Retries zu vermeiden.
    - F√ºhrt maximal `MAX_RETRIES` Wiederholungsversuche durch.
    - Loggt Versuche, Fehler und Wartezeiten in der Konsole.
- **Verwendung:** Diese Funktion wird aktuell von `convex/knowledgeBase.ts` f√ºr Embedding-Aufrufe genutzt und sollte f√ºr alle zuk√ºnftigen KI-Modellaufrufe verwendet werden.

## Weiterentwicklung

Geplante Erweiterungen:

- Integration weiterer Rechtsordnungen (Schweiz, EU-Recht)
- Verbesserung der Analysekonsistenz durch Finetuning der KI-Modelle
- Umfassendere Vertragsmanagement-Funktionen
- Exportm√∂glichkeiten f√ºr die Analysen
- Kollaborative Bearbeitung von Vertr√§gen

## Lizenz

Dieses Projekt ist unter der MIT-Lizenz lizenziert - siehe die LICENSE.md Datei f√ºr Details.
